# -*- coding: utf-8 -*-
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark import SparkConf
import re
from collections import Counter
import os
import sys
sys.stdout.reconfigure(encoding='utf-8')

os.environ["PYSPARK_PYTHON"] = r"D:\pyspark\env3\Scripts\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = r"D:\pyspark\env3\Scripts\python.exe"

class SholokhovAnalysis:
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("SholokhovTextAnalysis") \
            .master("local[4]") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
            .config("spark.python.worker.reuse", "false") \
            .config("spark.sql.adaptive.enabled", "false") \
            .config("spark.network.timeout", "600s") \
            .config("spark.executor.heartbeatInterval", "60s") \
            .config("spark.sql.broadcastTimeout", "1200s") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "false") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.bindAddress", "127.0.0.1") \
            .config("spark.driver.host", "127.0.0.1") \
            .getOrCreate()
        
        self.sc = self.spark.sparkContext
        
        # Русские стоп-слова
        self.stop_words = set([
            "и", "в", "во", "не", "что", "он", "на", "я", "с", "со", "как", "а", 
            "то", "все", "она", "так", "его", "но", "да", "ты", "к", "у", "же", 
            "вы", "за", "бы", "по", "только", "ее", "мне", "было", "вот", "от",
            "меня", "еще", "нет", "о", "из", "ему", "теперь", "когда", "даже",
            "ну", "вдруг", "ли", "если", "уже", "или", "ни", "быть", "был",
            "него", "до", "вас", "нибудь", "опять", "уж", "вам", "ведь", "там",
            "потом", "себя", "ничего", "ей", "может", "они", "тут", "где",
            "есть", "надо", "ней", "для", "мы", "тебя", "их", "чем", "была",
            "сам", "чтоб", "без", "будто", "чего", "раз", "тоже", "себе",
            "под", "будет", "ж", "тогда", "кто", "этот", "того", "потому",
            "этого", "какой", "совсем", "ним", "здесь", "этом", "один", "почти",
            "мой", "тем", "чтобы", "нее", "сейчас", "были", "куда", "зачем",
            "всех", "никогда", "можно", "при", "наконец", "два", "об", "другой",
            "хоть", "после", "над", "больше", "тот", "через", "эти", "нас",
            "про", "всего", "них", "какая", "много", "разве", "три", "эту",
            "моя", "впрочем", "хорошо", "свою", "этой", "перед", "иногда",
            "лучше", "чуть", "том", "нельзя", "такой", "им", "более", "всегда",
            "конечно", "всю", "между", "это", "всё"
        ])

    def read_local_file(self, file_path):
        """Чтение текста из локального файла с разными кодировками"""
        encodings = ['utf-8', 'windows-1251', 'cp1251', 'iso-8859-1', 'koi8-r']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    text = f.read()
                print(f"Файл успешно прочитан с кодировкой: {encoding}")
                return text
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"Ошибка при чтении файла с кодировкой {encoding}: {e}")
                continue
        
        print("Не удалось прочитать файл ни с одной из попробованных кодировок")
        return None
    
    def clean_text(self, text):
        """Очистка текста на драйвере, затем создание RDD"""
        lines = text.split('\n')
        all_cleaned_words = []
    
        for line in lines:
            words = re.findall(r'\b[а-яё]+\b', line.lower())
            cleaned_words = [
                word for word in words 
                if len(word) > 2 
                and word not in self.stop_words 
                and not word.isdigit()
            ]
            all_cleaned_words.extend(cleaned_words)
    
        # Создаем RDD из уже очищенных данных
        words_rdd = self.sc.parallelize(all_cleaned_words)
        return words_rdd

    def analyze_word_frequency(self, words_rdd, top_n=50):
        """Анализ частотности слов"""
        # Подсчет частот
        word_counts = words_rdd \
            .map(lambda word: (word, 1)) \
            .reduceByKey(lambda a, b: a + b)
        
        # Самые частые слова
        top_words = word_counts \
            .sortBy(lambda x: x[1], ascending=False) \
            .take(top_n)
        
        # Самые редкие слова
        rare_words = word_counts \
            .filter(lambda x: x[1] <= 2) \
            .sortBy(lambda x: x[1]) \
            .take(top_n)
        
        return top_words, rare_words
    
    def analyze_word_structure(self, words_rdd, top_n=50):
        """Анализ структуры слов"""
        # Выделение основы (первые 4 символа)
        stems_rdd = words_rdd.map(lambda word: word[:4] if len(word) > 4 else word)
        
        # Подсчет основ
        stem_counts = stems_rdd \
            .map(lambda stem: (stem, 1)) \
            .reduceByKey(lambda a, b: a + b)
        
        # Самые частые основы
        top_stems = stem_counts \
            .sortBy(lambda x: x[1], ascending=False) \
            .take(top_n)
        
        # Самые редкие основы
        rare_stems = stem_counts \
            .sortBy(lambda x: x[1]) \
            .take(top_n)
        
        return top_stems, rare_stems
    
    def generate_report(self, words_rdd, total_words):
        """Генерация статистического отчета"""
        unique_words = words_rdd.distinct().count()
        word_lengths = words_rdd.map(len)
        total_chars = word_lengths.sum()
        avg_word_length = total_chars / total_words
        
        print("\n" + "="*50)
        print("СТАТИСТИЧЕСКИЙ ОТЧЕТ")
        print("="*50)
        print(f"Общее количество слов: {total_words:,}")
        print(f"Уникальных слов: {unique_words:,}")
        print(f"Коэффициент уникальности: {unique_words/total_words*100:.2f}%")
        print(f"Средняя длина слова: {avg_word_length:.2f} букв")
        
        # Распределение по длине слов
        length_dist = words_rdd \
            .map(lambda word: (len(word), 1)) \
            .reduceByKey(lambda a, b: a + b) \
            .sortByKey() \
            .collect()
        
        print("\nРаспределение по длине слов:")
        for length, count in length_dist:
            print(f"  {length} букв: {count} слов")
    
    def run_analysis(self, file_path):
        """Запуск полного анализа с локальным файлом"""
        print("=== АНАЛИЗ ПРОИЗВЕДЕНИЯ 'СУДЬБА ЧЕЛОВЕКА' ===")
        print("Автор: Михаил Александрович Шолохов")
        
        # 1. Загрузка текста из локального файла
        print("\n1. ЗАГРУЗКА ТЕКСТА ИЗ ЛОКАЛЬНОГО ФАЙЛА...")
        text = self.read_local_file(file_path)
        if not text:
            print("Не удалось загрузить текст")
            return
        
        print(f"Загружено символов: {len(text):,}")
        
        # 2. Очистка текста
        print("\n2. ОЧИСТКА ТЕКСТА...")
        words_rdd = self.clean_text(text)
        words_rdd.cache() 
        
        # 3. Подсчет слов
        total_words = words_rdd.count()
        print(f"3. ОБЩЕЕ КОЛИЧЕСТВО СЛОВ: {total_words:,}")
        
        # 4. Анализ частотности
        print("\n4. АНАЛИЗ ЧАСТОТНОСТИ СЛОВ...")
        top_words, rare_words = self.analyze_word_frequency(words_rdd)
        
        print("\n50 САМЫХ ЧАСТЫХ СЛОВ:")
        print("-" * 30)
        for i, (word, count) in enumerate(top_words, 1):
            print(f"{i:2d}. {word:15} {count:5d}")
        
        print("\n50 САМЫХ РЕДКИХ СЛОВ:")
        print("-" * 30)
        for i, (word, count) in enumerate(rare_words, 1):
            print(f"{i:2d}. {word:15} {count:5d}")
        
        # 5. Анализ структуры
        print("\n5. АНАЛИЗ СТРУКТУРЫ СЛОВ...")
        top_stems, rare_stems = self.analyze_word_structure(words_rdd)
        
        print("\n50 САМЫХ ЧАСТЫХ ОСНОВ:")
        print("-" * 30)
        for i, (stem, count) in enumerate(top_stems, 1):
            print(f"{i:2d}. {stem:15} {count:5d}")
        
        print("\n50 САМЫХ РЕДКИХ ОСНОВ:")
        print("-" * 30)
        for i, (stem, count) in enumerate(rare_stems, 1):
            print(f"{i:2d}. {stem:15} {count:5d}")
        
        # 6. Статистический отчет
        self.generate_report(words_rdd, total_words)
        
        # Очистка кэша
        words_rdd.unpersist()
    
    def close(self):
        """Закрытие Spark сессии"""
        self.spark.stop()


if __name__ == "__main__":

    file_path = "D:/pyspark/env1/Scripts/sydba_cheloveka.txt" 

    analyzer = SholokhovAnalysis()
    try:
        analyzer.run_analysis(file_path)
    except Exception as e:
        print(f"Произошла ошибка: {e}")
        import traceback
        traceback.print_exc()
    finally:
        analyzer.close()
